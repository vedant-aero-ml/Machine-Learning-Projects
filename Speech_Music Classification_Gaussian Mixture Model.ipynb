{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a98dd9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import fft\n",
    "from scipy import linalg\n",
    "from scipy.linalg import eigh\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1743a4ff",
   "metadata": {},
   "source": [
    "### Spectrogram Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caeb6e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram(path):\n",
    "    data, sample_rate = librosa.load(path, sr = 16000) # Reading the file and decomposing into sample rate and amplitude vector\n",
    "    \n",
    "    # Defining Hamming window particulars\n",
    "    window = 0.025\n",
    "    shift = 0.01\n",
    "    \n",
    "    # Translating into number of samples from given Hamming window particulars\n",
    "    frame_length, frame_step = int(window * sample_rate), int(shift * sample_rate)\n",
    "    signal_length = len(data)\n",
    "\n",
    "    # Calculating number of frames. Converting to inn tas the values are floats and the calculated values need to be discrete integers.\n",
    "    num_frames = int((signal_length - frame_length) / frame_step)\n",
    "\n",
    "    # Creating an envelope of array that shifts with Hamming window \n",
    "    envelope_length = num_frames * num_frames + frame_length\n",
    "\n",
    "    # Storing data into a temporary array, to be used for appending\n",
    "    temp = np.zeros((envelope_length - signal_length))\n",
    "    envelope = np.append(data, temp) \n",
    "\n",
    "    # Using np.tile to create repeating arrays while using arange to set-up frame length and corresponding entries\n",
    "    storage = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "\n",
    "    # Storing the data into the frames\n",
    "    frames = envelope[storage]\n",
    "    \n",
    "    point_FFT = 64\n",
    "\n",
    "    fft_data = np.absolute(np.fft.fft(frames, point_FFT))  # Magnitude of the FFT\n",
    "    \n",
    "    fft_data = fft_data.T\n",
    "    \n",
    "    fft_data = fft_data[0:32,:] # Slicing into top 32 vectors \n",
    "    \n",
    "    spectrogram = fft_data.T - np.mean(fft_data.T, axis=0)\n",
    "\n",
    "    spectrogram = spectrogram.T\n",
    "    \n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e438e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing file names\n",
    "\n",
    "names_speech = os.listdir('speech_music_classification/train/speech/')\n",
    "\n",
    "names_music = os.listdir('speech_music_classification/train/music/')\n",
    "\n",
    "\n",
    "# Creating the music data matrix\n",
    "music_data = []\n",
    "\n",
    "for name in names_music:\n",
    "    music_data.append(spectrogram('speech_music_classification/train/music/'+name))\n",
    "\n",
    "music_data = np.array(music_data)\n",
    "\n",
    "music_data = music_data.reshape(music_data.shape[1], music_data.shape[2]*music_data.shape[0])\n",
    "music_data = music_data.T\n",
    "\n",
    "\n",
    "# Creating the speech data matrix\n",
    "speech_data = []\n",
    "\n",
    "for name in names_speech:\n",
    "    speech_data.append(spectrogram('speech_music_classification/train/speech/'+name))\n",
    "\n",
    "speech_data = np.array(speech_data)\n",
    "\n",
    "speech_data = speech_data.reshape(speech_data.shape[1], speech_data.shape[2]*speech_data.shape[0])\n",
    "speech_data = speech_data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fde76d",
   "metadata": {},
   "source": [
    "### KMeans Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8bd57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class KMeans(object):\n",
    "    def pairwise_dist(self, x, y): # get individual distances\n",
    "        xSumSquare = np.sum(np.square(x),axis=1);\n",
    "        ySumSquare = np.sum(np.square(y),axis=1);\n",
    "        mul = np.dot(x, y.T);\n",
    "        dists = np.sqrt(abs(xSumSquare[:, np.newaxis] + ySumSquare-2*mul))\n",
    "        return dists\n",
    "\n",
    "    def _init_centers(self, points, K, **kwargs): # compute centers\n",
    "        row, col = points.shape\n",
    "        retArr = np.empty([K, col])\n",
    "        for number in range(K):\n",
    "            randIndex = np.random.randint(row)\n",
    "            retArr[number] = points[randIndex]\n",
    "        \n",
    "        return retArr\n",
    "\n",
    "    def _update_assignment(self, centers, points): # update assignment of each data point of the particular cluster\n",
    "        row, col = points.shape\n",
    "        cluster_idx = np.empty([row])\n",
    "        distances = self.pairwise_dist(points, centers)\n",
    "        cluster_idx = np.argmin(distances, axis=1)\n",
    "\n",
    "        return cluster_idx\n",
    "\n",
    "    def _update_centers(self, old_centers, cluster_idx, points): # update cluster centroids based on new means\n",
    "        K, D = old_centers.shape\n",
    "        new_centers = np.empty(old_centers.shape)\n",
    "        for i in range(K):\n",
    "            new_centers[i] = np.mean(points[cluster_idx == i], axis = 0)\n",
    "        return new_centers\n",
    "\n",
    "    def _get_loss(self, centers, cluster_idx, points):  # find the loss value\n",
    "        dists = self.pairwise_dist(points, centers)\n",
    "        loss = 0.0\n",
    "        N, D = points.shape\n",
    "        for i in range(N):\n",
    "            loss = loss + np.square(dists[i][cluster_idx[i]])\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def __call__(self, points, K, max_iters=100, abs_tol=1e-16, rel_tol=1e-16, verbose=False, **kwargs): # implement KMeans on the data\n",
    "        centers = self._init_centers(points, K, **kwargs)\n",
    "        for it in range(max_iters):\n",
    "            cluster_idx = self._update_assignment(centers, points)\n",
    "            centers = self._update_centers(centers, cluster_idx, points)\n",
    "            loss = self._get_loss(centers, cluster_idx, points)\n",
    "            K = centers.shape[0]\n",
    "            if it:\n",
    "                diff = np.abs(prev_loss - loss)\n",
    "                if diff < abs_tol and diff / prev_loss < rel_tol:\n",
    "                    break\n",
    "            prev_loss = loss\n",
    "        return cluster_idx, centers, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d52c6",
   "metadata": {},
   "source": [
    "### EM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b50bdaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Defining Gaussian Function\n",
    "def Gaussian(x, mu, sigma):\n",
    "    d = len(x)  # Number of entries in the vector\n",
    "    N = (1/((2*np.pi)**(d/2)))*(np.exp(-0.5*(x-mu)@(np.linalg.inv(sigma))@((x-mu).T)))\n",
    "    return N\n",
    "\n",
    "\n",
    "#### Expectation step\n",
    "def E_step(data, weight, mu, sigma, K):\n",
    "    \n",
    "    n = data.shape[0]\n",
    "    \n",
    "    num = np.empty([n,K])\n",
    "    R = np.empty([n,K])\n",
    "    den = np.zeros(n)\n",
    "    \n",
    "    for n, sample in enumerate(data):\n",
    "        for k in range(K):\n",
    "            num[n][k] = weight[k]*Gaussian(sample, mu[k], sigma[k])\n",
    "            den[n] = den[n] + num[n][k]\n",
    "        \n",
    "        R[n] = num[n]/den[n]\n",
    "    \n",
    "    return R\n",
    "\n",
    "\n",
    "#### Maximization step\n",
    "def M_step(R,data, weight, mu, sigma):\n",
    "    \n",
    "    N = data.shape[0]\n",
    "    \n",
    "    D = data.shape[1]\n",
    "    \n",
    "    R = np.where(np.isnan(R), np.ma.array(R, mask=np.isnan(R)).mean(axis=0), R)  # Making sure that NaNs if created are replaced by mean of their columns\n",
    "    \n",
    "    assignment = []  # Array which stores the assigned cluster for all n data points\n",
    "    for sample in R:\n",
    "        assignment.append(np.argmax(sample))\n",
    "        \n",
    "    N = []  # N stores the count assigned for each Gaussian clusters\n",
    "    \n",
    "    for k in range(K):\n",
    "        N.append(assignment.count(k))\n",
    "        \n",
    "    \n",
    "    # Updating means\n",
    "    for k in range(K):\n",
    "        p = 0\n",
    "        for n in range(len(R)):\n",
    "            p += R[n][k]*data[n]\n",
    "        mu[k] = p/N[k]\n",
    "    \n",
    "        \n",
    "    # Updating covariance matrices\n",
    "    for k in range(K):\n",
    "        p = np.zeros([D,D])\n",
    "        for n in range(len(R)):\n",
    "            X = ((data[n]-mu[k])).reshape(D,1)\n",
    "            p +=(R[n][k]*X*X.T)\n",
    "        sigma[k] = p/N[k]\n",
    "        sigma[k]= np.diag(np.diag(sigma[k]))\n",
    "    \n",
    "    \n",
    "    # Updating weights\n",
    "    tot = len(data)\n",
    "    for k in range(K):\n",
    "        weight[k] = N[k]/tot\n",
    "        \n",
    "    \n",
    "    return assignment, weight, mu, sigma\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08c73e0",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "153f2b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(data, K):\n",
    "\n",
    "    n = data.shape[0]\n",
    "    d = data.shape[1]\n",
    "\n",
    "    # Initializing weights from priors\n",
    "    np.random.seed(42)\n",
    "    weight = [np.random.rand() for i in range(K)]\n",
    "    s = sum(weight)\n",
    "    weight = [ i/s for i in weight ]\n",
    "\n",
    "\n",
    "    # Initializing mean and sigma using K-Means algorithm\n",
    "    kmeans = KMeans(K)\n",
    "    identified_clusters = kmeans._call_(data)\n",
    "\n",
    "    mu = np.empty([K,d])\n",
    "    sigma = np.zeros([K,d,d])\n",
    "    for k in range(K):\n",
    "        tmp = data[np.argwhere(identified_clusters==k)]\n",
    "        tmp = tmp.reshape(tmp.shape[0],tmp.shape[2])\n",
    "        mu[k] = tmp.mean(axis=0)\n",
    "        sigma[k] = (tmp.T@tmp)/tmp.shape[0]\n",
    "        sigma[k]= np.diag(np.diag(sigma[k]))\n",
    "    \n",
    "    return weight, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b603d30",
   "metadata": {},
   "source": [
    "### Training GMM for Music for K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9be6017b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vedant\\AppData\\Local\\Temp/ipykernel_10320/4083499372.py:22: RuntimeWarning: invalid value encountered in true_divide\n",
      "  R[n] = num[n]/den[n]\n"
     ]
    }
   ],
   "source": [
    "K = 2\n",
    "weight_m, mu_m, sigma_m = initialize(music_data, K)\n",
    "R_m = E_step(music_data, weight_m, mu_m, sigma_m, K)\n",
    "assignment_m, weight_m, mu_m, sigma_m = M_step(R_m,music_data, weight_m, mu_m, sigma_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b3f75",
   "metadata": {},
   "source": [
    "### Training GMM for Speech for K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e18cb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vedant\\AppData\\Local\\Temp/ipykernel_10320/4083499372.py:22: RuntimeWarning: invalid value encountered in true_divide\n",
      "  R[n] = num[n]/den[n]\n"
     ]
    }
   ],
   "source": [
    "K = 2\n",
    "weight_s, mu_s, sigma_s = initialize(speech_data, K)\n",
    "R_s = E_step(speech_data, weight_s, mu_s, sigma_s, K)\n",
    "assignment_s, weight_s, mu_s, sigma_s = M_step(R_s,speech_data, weight_s, mu_s, sigma_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0f146",
   "metadata": {},
   "source": [
    "### Function to implement testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3f9ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(tst):\n",
    "    \n",
    "    likelihood_s = []   \n",
    "    for k in range(K):\n",
    "        for ele in t:\n",
    "            num = weight_s[k]*Gaussian(ele, mu_s[k], sigma_s[k])\n",
    "            likelihood_s.append(num)\n",
    "            \n",
    "    likelihood_m = []  \n",
    "    for k in range(K):\n",
    "        for ele in t:\n",
    "            num = weight_m[k]*Gaussian(ele, mu_m[k], sigma_m[k])\n",
    "            likelihood_m.append(num)\n",
    "            \n",
    "    if((sum(likelihood_s)/len(likelihood_s))>(sum(likelihood_m)/len(likelihood_m))):\n",
    "        return \"S\"\n",
    "    else:\n",
    "        return \"M\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8ec4e",
   "metadata": {},
   "source": [
    "The idea here is to calculate individual likelihood of each data point for each test file and average it over the columns. For whichever class the average value is higher, the final decision list will be appended accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8070d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_test = os.listdir('speech_music_classification/test/')\n",
    "\n",
    "test_data = []\n",
    "\n",
    "for name in names_test:\n",
    "    test_data.append((spectrogram('speech_music_classification/test/'+name)).T)\n",
    "    \n",
    "test_data = np.array(test_data)\n",
    "\n",
    "decision = []\n",
    "\n",
    "for data in test_data:\n",
    "    decision.append(test(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57331ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
